{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39aacb00-4768-4763-8989-d9d432ca57fd",
   "metadata": {},
   "source": [
    "# Review misclassified examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8d7a2-90c9-495e-a5b2-32713c3ce561",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc32ed-9bc4-48c1-b67c-397f40d7bbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be464c24-23e4-41ca-8146-1bb69da32e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fecd2f5-e4b4-4382-bba2-6de41a0e2f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb2a26-adf9-4fc3-88e5-2044206929c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/tmp/ml-pipeline/model/digit-recognition.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0afdd0-abd0-4075-960e-08fe55e0482f",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b7d5f-1855-456c-9001-0b6cdfae5891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import session\n",
    "import os\n",
    "\n",
    "key_id = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "bucket_name = os.environ.get(\"AWS_S3_BUCKET\")\n",
    "s3_endpoint = os.environ.get(\"AWS_S3_ENDPOINT\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=key_id, aws_secret_access_key=secret_key, endpoint_url=s3_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de7949-6314-4839-a6fc-72645896b911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val = np.load('/tmp/ml-pipeline/data/X_val.npy')\n",
    "y_val = np.load('/tmp/ml-pipeline/data/y_val.npy')\n",
    "X_train = np.load('/tmp/ml-pipeline/data/X_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c7dba-e010-4d9f-85ff-79f3cb28b7ff",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6860fd-6620-42b4-a3f5-729c4eb840a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2d4df-b148-4ec9-83f0-6e15ce6f7acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_val_true = np.argmax(y_val,axis=1)\n",
    "y_val_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd97b4-b5db-4b58-85e3-ecc54de5acb7",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28045a9-e318-4467-8da4-16091f517d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.argmax(y_val,axis=1), y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7b876-2c50-4b08-a2ad-43803271e7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_index = cm\n",
    "np.fill_diagonal(cm_index,0)\n",
    "cm_index = np.where(cm_index==cm_index.max())\n",
    "if len(cm_index)>1:\n",
    "    cm_index = [cm_index[0][0],cm_index[1][0]]\n",
    "cm_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d376d-4886-4c60-be64-ff4461102474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cm_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1a5fa-4b29-4c25-a962-3d8e7f4b01dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nine_incorrect_examples = X_val[(y_val_true==cm_index[0]) & (y_val_pred==cm_index[1])]\n",
    "nine_incorrect_examples = nine_incorrect_examples.reshape(-1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c3b71-7549-4dcb-b8aa-58350e070c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filters_layer_1 = model.layers[1].get_weights()[0]\n",
    "filters_layer_1 = np.squeeze(filters_layer_1).reshape(32,5,5)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.suptitle('Missclassified examples')\n",
    "w = 10\n",
    "h = 10\n",
    "columns = 6\n",
    "rows = 6\n",
    "for i in range(columns*rows +1):\n",
    "    if i==nine_incorrect_examples.shape[0]:\n",
    "        break\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(nine_incorrect_examples[i], cmap='Greys')\n",
    "    plt.title('True:{},Pred:{}'.format(cm_index[0],cm_index[1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d8388-7d81-4ff3-b11c-9e3b857e61e4",
   "metadata": {},
   "source": [
    "> <div class=\"alert alert-block alert-info\">\n",
    "<b>Looking at the missclassifications, its understandable why the model was unable classify these examples correctly. Some of the examples are quite ambiguous, even a human labeller would probably be unable to clearly label them with good confidence. However, it does seem that there is room for improvement for some of the examples. With the use of data augmentation and hyper-parameter tuning, we should be able to further improve performance.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b66c9a-3a7e-45a2-8b25-a62406f38799",
   "metadata": {},
   "source": [
    "> <div class=\"alert alert-block alert-info\">\n",
    "<b>Before we try these additional techniques, let's take a quick look at the learnt convolutional filters and feature maps, which should give us some insight on how the network is learning.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f86c4-fb96-42a8-8d14-92796ff059eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604921f-f290-43cd-aac2-b16d8e8587b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba415b-edcd-4997-955b-f7699315da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_layer_1 = model.layers[1].get_weights()[0]\n",
    "filters_layer_1 = np.squeeze(filters_layer_1).reshape(32,5,5)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.suptitle('CNN 1 FILTERS')\n",
    "w = 10\n",
    "h = 10\n",
    "columns = 6\n",
    "rows = 6\n",
    "for i in range(columns*rows +1):\n",
    "    if i==filters_layer_1.shape[0]:\n",
    "        break\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(filters_layer_1[i], cmap='Greys')\n",
    "    plt.title(i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653f28d-4e38-4359-b3f4-98b2cd8d2324",
   "metadata": {},
   "source": [
    "> <div class=\"alert alert-block alert-info\">\n",
    "<b>Shown above is the visualizatins of the 32 learnt filters in the first convolutional layer. The filters are what convolve over the images to produce the activation maps, also known as feature-maps.</b>\n",
    "    <br></br>\n",
    "<b>The feature-maps capture key features and patterns from the input (i.e the input image or feature maps from previous ConvBlocks).</b>\n",
    "    <br></br> \n",
    "<b>The reason for visualizing feature maps is to understand what features of the input are detected and preserved in the feature maps. Lets now explore some of the feature maps to see what the model is capturing.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242d08e-5a2b-44d9-bfde-23c6307b6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # summarize output shape\n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334ff93-390d-432a-a22c-7fda8c06ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "fm_model = keras.Model(inputs=model.input, outputs=successive_outputs)\n",
    "\n",
    "successive_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf228526-38e1-4f22-b31f-ac6693943bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = X_train[[9]]\n",
    "plt.imshow(test_example[0], cmap='Greys')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "successive_feature_maps = fm_model.predict(test_example)\n",
    "len(successive_feature_maps) # 12 for 12 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ded7e-e1c4-40b0-9b44-a8b54cff786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model.layers]\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    print(layer_name)\n",
    "    if len(feature_map.shape) == 4:\n",
    "        n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "        size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "\n",
    "        # Postprocess the feature to be visually palatable\n",
    "        for i in range(n_features):\n",
    "            x  = feature_map[0, :, :, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std ()\n",
    "            x *=  64\n",
    "            x += 128\n",
    "            x  = np.clip(x, 0, 255).astype('uint8')\n",
    "            # Tile each filter into a horizontal grid\n",
    "            display_grid[:, i * size : (i + 1) * size] = x\n",
    "            \n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure( figsize=(scale * n_features, scale) )\n",
    "        plt.title(layer_name) \n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='Greys' )\n",
    "        plt.colorbar()\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60378658-5d48-4142-8644-c1696be90f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
